{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import sys, os\n",
    "import pystk\n",
    "import ray\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('device = ', device)\n",
    "ray.init(logging_level=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Rollout:\n",
    "    def __init__(self, screen_width, screen_height, hd=True, track='lighthouse', render=True, frame_skip=1):\n",
    "        # Init supertuxkart\n",
    "        if not render:\n",
    "            config = pystk.GraphicsConfig.none()\n",
    "        elif hd:\n",
    "            config = pystk.GraphicsConfig.hd()\n",
    "        else:\n",
    "            config = pystk.GraphicsConfig.ld()\n",
    "        config.screen_width = screen_width\n",
    "        config.screen_height = screen_height\n",
    "        pystk.init(config)\n",
    "        \n",
    "        self.frame_skip = frame_skip\n",
    "        self.render = render\n",
    "        race_config = pystk.RaceConfig(track=track)\n",
    "        self.race = pystk.Race(race_config)\n",
    "        self.race.start()\n",
    "    \n",
    "    def __call__(self, agent, n_steps=200):\n",
    "        torch.set_num_threads(1)\n",
    "        self.race.restart()\n",
    "        self.race.step()\n",
    "        data = []\n",
    "        track_info = pystk.Track()\n",
    "        track_info.update()\n",
    "\n",
    "        for i in range(n_steps // self.frame_skip):\n",
    "            world_info = pystk.WorldState()\n",
    "            world_info.update()\n",
    "\n",
    "            # Gather world information\n",
    "            kart_info = world_info.players[0].kart\n",
    "\n",
    "            agent_data = {'track_info': track_info, 'kart_info': kart_info}\n",
    "            if self.render:\n",
    "                agent_data['image'] = np.array(self.race.render_data[0].image)\n",
    "\n",
    "            # Act\n",
    "            action = agent(**agent_data)\n",
    "            agent_data['action'] = action\n",
    "\n",
    "            # Take a step in the simulation\n",
    "            for it in range(self.frame_skip):\n",
    "                self.race.step(action)\n",
    "\n",
    "            # Save all the relevant data\n",
    "            data.append(agent_data)\n",
    "        return data\n",
    "\n",
    "def show_video(frames, fps=30):\n",
    "    import imageio\n",
    "    from IPython.display import Video, display \n",
    "    \n",
    "    imageio.mimwrite('/tmp/test.mp4', frames, fps=fps, bitrate=1000000)\n",
    "    display(Video('/tmp/test.mp4', width=800, height=600, embed=True))\n",
    "\n",
    "viz_rollout = Rollout.remote(400, 300)\n",
    "def show_agent(agent, n_steps=600):\n",
    "    data = ray.get(viz_rollout.__call__.remote(agent, n_steps=n_steps))\n",
    "    show_video([d['image'] for d in data])\n",
    "    \n",
    "rollouts = [Rollout.remote(50, 50, hd=False, render=False, frame_skip=5) for i in range(10)]\n",
    "def rollout_many(many_agents, **kwargs):\n",
    "    ray_data = []\n",
    "    for i, agent in enumerate(many_agents):\n",
    "         ray_data.append( rollouts[i % len(rollouts)].__call__.remote(agent, **kwargs) )\n",
    "    return ray.get(ray_data)\n",
    "\n",
    "def dummy_agent(**kwargs):\n",
    "    action = pystk.Action()\n",
    "    action.acceleration = 1\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_points_on_track(distance, track):\n",
    "    distance = np.clip(distance, track.path_distance[0,0], track.path_distance[-1,1]).astype(np.float32)\n",
    "    valid_node = (track.path_distance[..., 0] <= distance) & (distance <= track.path_distance[..., 1])\n",
    "    valid_node_idx, = np.where(valid_node)\n",
    "    node_idx = valid_node_idx[0] # np.random.choice(valid_node_idx)\n",
    "    d = track.path_distance[node_idx].astype(np.float32)\n",
    "    x = track.path_nodes[node_idx][:,[0,2]].astype(np.float32) # Ignore the y coordinate\n",
    "    w, = track.path_width[node_idx].astype(np.float32)\n",
    "    \n",
    "    t = (distance - d[0]) / (d[1] - d[0])\n",
    "    mid = x[1] * t + x[0] * (1 - t)\n",
    "    x10 = (x[1] - x[0]) / np.linalg.norm(x[1]-x[0])\n",
    "    x10_ortho = np.array([-x10[1],x10[0]], dtype=float32)\n",
    "    return mid - w / 2 * x10_ortho, mid, mid + w / 2 * x10_ortho\n",
    "    \n",
    "\n",
    "def state_features(track_info, kart_info, absolute=False, **kwargs):\n",
    "    f = np.concatenate([three_points_on_track(kart_info.distance_down_track + d, track_info) for d in [0,5,10,15,20]])\n",
    "    if absolute:\n",
    "        return f\n",
    "    p = np.array(kart_info.location)[[0,2]].astype(np.float32)\n",
    "    t = np.array(kart_info.front)[[0,2]].astype(np.float32)\n",
    "    f = f - p[None]\n",
    "    d = (p-t) / np.linalg.norm(p-t)\n",
    "    d_o = np.array([-d[1], d[0]], dtype=float32)\n",
    "    return np.stack([f.dot(d), f.dot(d_o)], axis=1)\n",
    "    \n",
    "# Let's load a fancy auto-pilot. You'll write one yourself in your homework.\n",
    "from _auto_pilot import auto_pilot\n",
    "data, = rollout_many([auto_pilot], n_steps=400)\n",
    "\n",
    "figure()\n",
    "f = state_features(**data[50])\n",
    "plot(f[:,1].flat, f[:,0].flat, '*')\n",
    "axis('equal')\n",
    "gca().invert_yaxis()\n",
    "\n",
    "figure()\n",
    "for d in data:\n",
    "    f = state_features(**d, absolute=True)\n",
    "    plot(f[:,1].flat, f[:,0].flat, '*')\n",
    "axis('equal')\n",
    "gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Bernoulli\n",
    "\n",
    "def new_action_net():\n",
    "    return torch.nn.Linear(2*5*3, 1, bias=False)\n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, action_net):\n",
    "        self.action_net = action_net.cpu().eval()\n",
    "    \n",
    "    def __call__(self, track_info, kart_info, **kwargs):\n",
    "        f = state_features(track_info, kart_info)\n",
    "        output = self.action_net(torch.as_tensor(f).view(1,-1))[0]\n",
    "\n",
    "        action = pystk.Action()\n",
    "        action.acceleration = 1\n",
    "        steer_dist = Bernoulli(logits=output[0])\n",
    "        action.steer = steer_dist.sample()*2-1\n",
    "        return action\n",
    "\n",
    "class GreedyActor:\n",
    "    def __init__(self, action_net):\n",
    "        self.action_net = action_net.cpu().eval()\n",
    "    \n",
    "    def __call__(self, track_info, kart_info, **kwargs):\n",
    "        f = state_features(track_info, kart_info)\n",
    "        output = self.action_net(torch.as_tensor(f).view(1,-1))[0]\n",
    "\n",
    "        action = pystk.Action()\n",
    "        action.acceleration = 1\n",
    "        action.steer = output[0]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_net = new_action_net()\n",
    "show_agent(Actor(action_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_action_nets = [new_action_net() for i in range(10)]\n",
    "\n",
    "data = rollout_many([Actor(action_net) for action_net in many_action_nets], n_steps=600)\n",
    "\n",
    "good_initialization = many_action_nets[ np.argmax([d[-1]['kart_info'].overall_distance for d in data]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_agent(Actor(good_initialization))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall what we're trying to do in RL: maximize the expected return of a policy $\\pi$ (or in turn minmize a los $L$)\n",
    "$$\n",
    "-L = E_{\\tau \\sim P_\\pi}[R(\\tau)],\n",
    "$$\n",
    "where $\\tau = \\{s_0, a_0, s_1, a_1, \\ldots\\}$ is a trajectory of states and actions.\n",
    "The return of a trajectory is then defined as the sum of individual rewards $R(\\tau) = \\sum_k r(s_k)$ (we won't discount in this assignment).\n",
    "\n",
    "Policy gradient computes the gradient of the loss $L$ using the log-derivative trick\n",
    "$$\n",
    "\\nabla_\\pi L = -E_{\\tau \\sim P_\\pi}[\\sum_k r(s_k) \\nabla_\\pi \\sum_i \\log \\pi(a_i | s_i)].\n",
    "$$\n",
    "Since the return $r(s_k)$ only depends on action $a_i$ in the past $i < k$ we can further simplify the above equation:\n",
    "$$\n",
    "\\nabla_\\pi L = -E_{\\tau \\sim P_\\pi}\\left[\\sum_i \\left(\\nabla_\\pi \\log \\pi(a_i | s_i)\\right)\\left(\\sum_{k=i}^{i+T} r(s_k) \\right)\\right].\n",
    "$$\n",
    "We will implement an estimator for this objective below. There are a few steps that we need to follow:\n",
    "\n",
    " * The expectation $E_{\\tau \\sim P_\\pi}$ are rollouts of our policy\n",
    " * The log probability $\\log \\pi(a_i | s_i)$ uses the `Categorical.log_prob`\n",
    " * Gradient computation uses the `.backward()` function\n",
    " * The gradient $\\nabla_\\pi L$ is then used in a standard optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "n_epochs = 20\n",
    "n_trajectories = 10\n",
    "n_iterations =50\n",
    "batch_size = 128\n",
    "T = 20\n",
    "\n",
    "action_net = copy.deepcopy(good_initialization)\n",
    "\n",
    "best_action_net = copy.deepcopy(action_net)\n",
    "\n",
    "optim = torch.optim.Adam(action_net.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    eps = 1e-2\n",
    "    \n",
    "    # Roll out the policy, compute the Expectation\n",
    "    trajectories = rollout_many([Actor(action_net)]*n_trajectories, n_steps=600)\n",
    "    print('epoch = %d   best_dist = '%epoch, np.max([t[-1]['kart_info'].overall_distance for t in trajectories]))\n",
    "    \n",
    "    # Compute all the reqired quantities to update the policy\n",
    "    features = []\n",
    "    returns = []\n",
    "    actions = []\n",
    "    for trajectory in trajectories:\n",
    "        for i in range(len(trajectory)):\n",
    "            # Compute the returns\n",
    "            returns.append( trajectory[min(i+T, len(trajectory)-1)]['kart_info'].overall_distance -\n",
    "                            trajectory[i]['kart_info'].overall_distance )\n",
    "            # Compute the features\n",
    "            features.append( torch.as_tensor(state_features(**trajectory[i]), dtype=torch.float32).cuda().view(-1) )\n",
    "            # Store the action that we took\n",
    "            actions.append( trajectory[i]['action'].steer > 0 )\n",
    "    \n",
    "    # Upload everything to the GPU\n",
    "    returns = torch.as_tensor(returns, dtype=torch.float32).cuda()\n",
    "    actions = torch.as_tensor(actions, dtype=torch.float32).cuda()\n",
    "    features = torch.stack(features).cuda()\n",
    "    \n",
    "    returns = (returns - returns.mean()) / returns.std()\n",
    "    \n",
    "    action_net.train().cuda()\n",
    "    avg_expected_log_return = []\n",
    "    for it in range(n_iterations):\n",
    "        batch_ids = torch.randint(0, len(returns), (batch_size,), device=device)\n",
    "        batch_returns = returns[batch_ids]\n",
    "        batch_actions = actions[batch_ids]\n",
    "        batch_features = features[batch_ids]\n",
    "        \n",
    "        output = action_net(batch_features)\n",
    "        pi = Bernoulli(logits=output[:,0])\n",
    "        \n",
    "        expected_log_return = (pi.log_prob(batch_actions)*batch_returns).mean()\n",
    "        optim.zero_grad()\n",
    "        (-expected_log_return).backward()\n",
    "        optim.step()\n",
    "        avg_expected_log_return.append(float(expected_log_return))\n",
    "\n",
    "    best_performance, current_performance = rollout_many([GreedyActor(best_action_net), GreedyActor(action_net)], n_steps=600)\n",
    "    if best_performance[-1]['kart_info'].overall_distance < current_performance[-1]['kart_info'].overall_distance:\n",
    "        best_action_net = copy.deepcopy(action_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_agent(GreedyActor(best_action_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}